# Voice Evaluater - Comprehensive Audio Assessment System

A sophisticated voice evaluation system that analyzes speech across multiple dimensions using Azure AI services, HuggingFace models, and advanced acoustic analysis.

## ğŸ¯ Features

### Multi-Model Analysis Pipeline

```
Audio Input
  â”‚
  â”œâ”€ ğŸ¤ Azure Speech SDK
  â”‚    â”œâ”€ Transcript
  â”‚    â”œâ”€ Word timings
  â”‚    â”œâ”€ Pronunciation accuracy
  â”‚    â”œâ”€ Word errors
  â”‚    â”œâ”€ Completeness
  â”‚    â””â”€ Micro-fluency
  â”‚
  â”œâ”€ ğŸ¤– Azure OpenAI LLM
  â”‚    â”œâ”€ Grammar quality
  â”‚    â”œâ”€ Sentence formation
  â”‚    â”œâ”€ Content depth
  â”‚    â”œâ”€ Answer relevance
  â”‚    â”œâ”€ Professional tone
  â”‚    â””â”€ AI Feedback
  â”‚
  â”œâ”€ ğŸŒ MTI Accent Analyzer
  â”‚    â”œâ”€ Accent detection
  â”‚    â””â”€ MTI impact score
  â”‚
  â”œâ”€ ğŸ’ª Confidence Analyzer
  â”‚    â”œâ”€ Acoustic confidence (WavLM)
  â”‚    â”‚   â”œâ”€ Speech rate stability
  â”‚    â”‚   â”œâ”€ Pause regularity
  â”‚    â”‚   â”œâ”€ Pitch variance
  â”‚    â”‚   â”œâ”€ Energy consistency
  â”‚    â”‚   â””â”€ Completion score
  â”‚    â””â”€ Linguistic confidence (MPNet)
  â”‚         â”œâ”€ Semantic coherence
  â”‚         â””â”€ Content complexity
  â”‚
  â””â”€ ğŸ“Š Final Scoring & Report
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.9+
- ffmpeg (for audio conversion)
- Azure Speech Services subscription
- Azure OpenAI subscription

### Installation

1. **Clone or navigate to the project:**
```bash
cd /home/full-stack/J2W/voice_evaluater
```

2. **Install dependencies:**
```bash
uv pip install -e .
# or with pip
pip install -e .
```

3. **Install ffmpeg (if not already installed):**
```bash
# Ubuntu/Debian
sudo apt-get install ffmpeg

# macOS
brew install ffmpeg
```

4. **Configure environment variables:**
```bash
cp .env.example .env
# Edit .env with your Azure credentials
```

Required configuration in `.env`:
```env
AZURE_SPEECH_KEY=your_azure_speech_key
AZURE_SPEECH_REGION=your_region  # e.g., eastus
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_KEY=your_openai_key
AZURE_OPENAI_DEPLOYMENT=your_deployment_name  # e.g., gpt-4
```

## ğŸ“– Usage

### Basic Usage

Place your audio file in the `audio_files` directory and run:

```bash
python main.py audio_files/your_audio.mp3
```

### With Context

Provide context about the speech for better analysis:

```bash
python main.py audio_files/interview.wav --context "Job interview response about teamwork"
```

### Supported Audio Formats

- MP3
- WAV
- M4A
- FLAC
- OGG
- Any format supported by ffmpeg

## ğŸ“‚ Project Structure

```
voice_evaluater/
â”œâ”€â”€ main.py                 # Main orchestrator
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py          # Configuration management
â”‚   â”œâ”€â”€ utils.py           # Utility functions
â”‚   â”œâ”€â”€ azure_speech.py    # Azure Speech SDK integration
â”‚   â”œâ”€â”€ azure_llm.py       # Azure OpenAI integration
â”‚   â”œâ”€â”€ mti_analyzer.py    # MTI/Accent analysis
â”‚   â”œâ”€â”€ confidence_analyzer.py  # Confidence assessment
â”‚   â””â”€â”€ report_generator.py    # Report generation
â”œâ”€â”€ audio_files/           # Input audio files (place your files here)
â”œâ”€â”€ reports/               # Generated JSON reports
â”œâ”€â”€ .env                   # Your configuration (create from .env.example)
â”œâ”€â”€ .env.example          # Configuration template
â”œâ”€â”€ pyproject.toml        # Project dependencies
â””â”€â”€ README.md             # This file
```

## ğŸ¯ Models Used

### 1. Azure Speech SDK
- **Purpose**: Transcription, pronunciation, fluency
- **Cost**: ~$1/hour (5 hours free/month)

### 2. Azure OpenAI
- **Purpose**: Grammar, content, linguistic quality
- **Model**: GPT-4 or GPT-3.5-Turbo

### 3. Accent Analyzer
- **Model**: `Jzuluaga/accent-id-commonaccent_xlsr-en-english`
- **Purpose**: MTI impact and accent detection
- **Source**: HuggingFace

### 4. Acoustic Confidence
- **Model**: `microsoft/wavlm-large`
- **Purpose**: Delivery stability analysis
- **Features**: Speech rate, pauses, pitch, energy

### 5. Linguistic Confidence
- **Model**: `sentence-transformers/all-mpnet-base-v2`
- **Purpose**: Content quality and coherence
- **Features**: Semantic coherence, complexity

## ğŸ“Š Output

The system generates:

1. **Console Summary**: Real-time progress and key metrics
2. **JSON Report**: Detailed analysis saved to `reports/` directory
3. **Comprehensive Scores**:
   - Final Score (0-100)
   - Category Scores (Speech, Linguistic, Confidence, Accent)
   - Component Scores (10+ individual metrics)
   - Letter Grade (A+ to D)
   - Performance Level

### Example Output

```
ğŸ† FINAL SCORE: ğŸŸ¢ 85.3/100 - A (Excellent)

ğŸ“Š Category Scores:
  â€¢ Speech Quality:       ğŸŸ¢ 88.5/100
  â€¢ Linguistic Quality:   ğŸŸ¢ 84.2/100
  â€¢ Confidence:           ğŸŸ¢ 82.7/100
  â€¢ Accent Clarity:       ğŸŸ¢ 86.0/100

ğŸŒ Accent Analysis:
  â€¢ Detected: american (confidence: 87.3%)

ğŸ’¬ AI Feedback:
  Your speech demonstrates strong grammar and clear articulation...
```

## ğŸ› ï¸ Troubleshooting

### Model Download Issues

First-time use will download models (~2-3GB total). Ensure stable internet connection.

### Audio Format Issues

If audio conversion fails, ensure ffmpeg is properly installed:
```bash
ffmpeg -version
```

### Azure Connection Issues

Verify your credentials:
```bash
# Test Azure Speech
python -c "import azure.cognitiveservices.speech as speechsdk; print('Azure SDK OK')"

# Test Azure OpenAI
python -c "from openai import AzureOpenAI; print('OpenAI SDK OK')"
```

## ğŸ’¡ Tips

- **Best Audio Quality**: Use WAV files with 16kHz, mono for optimal results
- **Context Matters**: Providing context improves LLM analysis accuracy
- **First Run**: Initial model downloads may take several minutes
- **Cost Tracking**: Check `reports/` for cost estimates per analysis

## ğŸ“ License

This project uses Azure AI services and HuggingFace models. Ensure compliance with their respective licenses.

## ğŸ¤ Contributing

This is a custom evaluation system. For improvements or issues, modify the relevant module in `src/`.

## ğŸ“§ Support

For Azure-related issues:
- [Azure Speech Documentation](https://docs.microsoft.com/azure/cognitive-services/speech-service/)
- [Azure OpenAI Documentation](https://docs.microsoft.com/azure/cognitive-services/openai/)

For model-specific questions:
- [HuggingFace Model Hub](https://huggingface.co/models)
